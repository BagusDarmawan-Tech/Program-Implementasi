{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8Uh5ql9UhxX"
      },
      "source": [
        "Unsupervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JEUNuqbYuvq"
      },
      "source": [
        "Topic Modeling dengan LDA (Latent Dirichlet Allocation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc_BjRxNTTxw",
        "outputId": "c74d0cb2-66cf-4bda-93fb-f1f334661757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topik #1:\n",
            "['know', 'pain', 'really', 'started', 'time', 'don', 'feel', 'like', 'just', 'anxiety']\n",
            "\n",
            "Topik #2:\n",
            "['working', 'college', 'year', 'money', 'don', 'just', 'school', 'time', 'work', 'job']\n",
            "\n",
            "Topik #3:\n",
            "['depression', 'body', 'eat', 'want', 'sleep', 'amp', 'just', 'feel', 'day', 'like']\n",
            "\n",
            "Topik #4:\n",
            "['diagnosed', 'know', 'people', 'health', 'therapy', 'mental', 'bipolar', 'anxiety', 'help', 'depression']\n",
            "\n",
            "Topik #5:\n",
            "['know', 'just', 'got', 'parents', 'did', 'time', 'year', 'family', 'years', 'life']\n",
            "\n",
            "Topik #6:\n",
            "['know', 'like', 'time', 'really', 'got', 'told', 'friend', 'said', 'did', 'just']\n",
            "\n",
            "Topik #7:\n",
            "['going', 'die', 'fucking', 'feel', 'like', 'know', 'anymore', 'life', 'want', 'just']\n",
            "\n",
            "Topik #8:\n",
            "['want', 'life', 'think', 'really', 'don', 'know', 'people', 'just', 'feel', 'like']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"Combined Data.csv\")\n",
        "df = df.dropna(subset=[\"statement\"])\n",
        "\n",
        "# Preprocessing sederhana\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower().split()\n",
        "    text = [word for word in text if word not in CountVectorizer(stop_words='english').get_stop_words() and len(word) > 2]\n",
        "    return ' '.join(text)\n",
        "\n",
        "df['clean_text'] = df['statement'].apply(preprocess)\n",
        "\n",
        "# Vectorisasi\n",
        "vectorizer = CountVectorizer(max_df=0.9, min_df=10, stop_words='english')\n",
        "doc_term_matrix = vectorizer.fit_transform(df['clean_text'])\n",
        "\n",
        "# LDA\n",
        "lda = LatentDirichletAllocation(n_components=8, random_state=42)\n",
        "lda.fit(doc_term_matrix)\n",
        "\n",
        "# Tampilkan topik\n",
        "for idx, topic in enumerate(lda.components_):\n",
        "    print(f\"Topik #{idx+1}:\")\n",
        "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R993sutbx_J",
        "outputId": "64aeb576-b353-400c-81a7-e8fb93c30d5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWDvOQvadztB",
        "outputId": "7b0227d3-89f9-45d2-98e4-79f03f39f3fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topik #1: ['want', 'life', 'day', 'nothing', 'hate', 'tired', 'anything', 'going', 'live', 'everything']\n",
            "Topik #2: ['amp', 'experience', 'https', 'either', 'physical', 'com', 'gonna', 'losing', 'wanna', 'exhausted']\n",
            "Topik #3: ['work', 'years', 'time', 'year', 'job', 'get', 'life', 'ago', 'school', 'back']\n",
            "Topik #4: ['mom', 'put', 'got', 'said', 'dad', 'kill', 'house', 'energy', 'reading', 'death']\n",
            "Topik #5: ['bipolar', 'take', 'fucking', 'manic', 'effects', 'inside', 'realized', 'lamictal', 'lithium', 'vent']\n",
            "Topik #6: ['anxiety', 'day', 'get', 'also', 'got', 'feeling', 'stress', 'time', 'started', 'back']\n",
            "Topik #7: ['help', 'depression', 'might', 'please', 'need', 'deal', 'guess', 'depressed', 'self', 'due']\n",
            "Topik #8: ['like', 'feel', 'know', 'people', 'even', 'really', 'think', 'things', 'would', 'get']\n",
            "\n",
            "Coherence Score: 0.3459\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords jika belum\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# 1. Load data\n",
        "df = pd.read_csv(\"Combined Data.csv\")\n",
        "df = df.dropna(subset=[\"statement\"])\n",
        "\n",
        "# 2. Preprocessing\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Hapus simbol dan angka\n",
        "    text = text.lower().split()  # Lowercase + split\n",
        "    text = [word for word in text if word not in stop_words and len(word) > 2]\n",
        "    return text\n",
        "\n",
        "# 3. Tokenisasi\n",
        "tokenized_docs = df['statement'].apply(preprocess).tolist()\n",
        "\n",
        "# 4. Buat dictionary dan corpus\n",
        "id2word = corpora.Dictionary(tokenized_docs)\n",
        "corpus = [id2word.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "# 5. Latih LDA model\n",
        "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=8, random_state=42, passes=20)\n",
        "\n",
        "# 6. Tampilkan topik\n",
        "for i, topic in lda_model.show_topics(formatted=False, num_words=10):\n",
        "    print(f\"Topik #{i+1}: {[word for word, _ in topic]}\")\n",
        "print()\n",
        "\n",
        "# 7. Hitung Coherence Score\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=tokenized_docs, dictionary=id2word, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print(f\"Coherence Score: {coherence_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jRozNN9DgeA_",
        "outputId": "e88c5ced-8143-4ee7-b5e6-1cbec8efd7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topik #1: ['amp', 'saying', 'idk', 'health', 'energy', 'appointment', 'study', 'write', 'completely', 'suffering']\n",
            "Topik #2: ['talk', 'school', 'friend', 'depressed', 'pretty', 'mom', 'shit', 'help', 'depression', 'parents']\n",
            "Topik #3: ['therapy', 'wish', 'avpd', 'trying', 'deep', 'look', 'wanna', 'handle', 'cut', 'kid']\n",
            "Topik #4: ['stress', 'sleep', 'recently', 'constantly', 'literally', 'night', 'high', 'super', 'completely', 'second']\n",
            "Topik #5: ['anxiety', 'like', 'feeling', 'depression', 'feel', 'anxious', 'help', 'having', 'symptoms', 'bad']\n",
            "Topik #6: ['like', 'feel', 'know', 'people', 'life', 'think', 'time', 'things', 'want', 'way']\n",
            "Topik #7: ['work', 'got', 'time', 'job', 'year', 'ago', 'went', 'day', 'going', 'told']\n",
            "Topik #8: ['want', 'anymore', 'life', 'live', 'tired', 'feel', 'like', 'pain', 'die', 'living']\n",
            "\n",
            "Coherence Score: 0.3803\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"Combined Data.csv\")\n",
        "df = df.dropna(subset=[\"statement\"])\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower().split()\n",
        "    return [word for word in text if word not in STOPWORDS and len(word) > 2]\n",
        "\n",
        "# Tokenize\n",
        "tokenized_docs = df['statement'].apply(preprocess).tolist()\n",
        "\n",
        "# Buat bigram dan trigram\n",
        "bigram = Phrases(tokenized_docs, min_count=5, threshold=80)\n",
        "trigram = Phrases(bigram[tokenized_docs], threshold=80)\n",
        "bigram_mod = Phraser(bigram)\n",
        "trigram_mod = Phraser(trigram)\n",
        "\n",
        "# Transformasi ke bigram dan trigram\n",
        "data_words_trigrams = [trigram_mod[bigram_mod[doc]] for doc in tokenized_docs]\n",
        "\n",
        "# Buat dictionary dan corpus\n",
        "id2word = corpora.Dictionary(data_words_trigrams)\n",
        "corpus = [id2word.doc2bow(text) for text in data_words_trigrams]\n",
        "\n",
        "# Latih model LDA\n",
        "lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=8, random_state=42, passes=20)\n",
        "\n",
        "# Hitung coherence score\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=data_words_trigrams, dictionary=id2word, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "# Tampilkan topik\n",
        "for i, topic in lda_model.show_topics(formatted=False, num_words=10):\n",
        "    print(f\"Topik #{i+1}: {[word for word, _ in topic]}\")\n",
        "print(f\"\\nCoherence Score: {coherence_score:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}